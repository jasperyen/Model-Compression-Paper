# Model-Compression-Paper

Model Compression
---
- jcyan 2016 ICLR [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/pdf/1510.00149)
- jcyan 2017 ICCV [Learning Efficient Convolutional Networks through Network Slimming](https://arxiv.org/pdf/1708.06519)
- jcyan 2018 ECCV [AMC: AutoML for Model Compression and Acceleration on Mobile Devices](https://arxiv.org/pdf/1802.03494)
- jcyan 2018 ECCV [Data-Driven Sparse Structure Selection for Deep Neural Networks](https://arxiv.org/pdf/1707.01213)
- jcyan 2019 ICLR [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/pdf/1803.03635)
- jcyan 2019 CVPR [HAQ: Hardware-Aware Automated Quantization with Mixed Precision](https://arxiv.org/pdf/1811.08886)
- jcyan 2019 ICLR [Rethinking the Value of Network Pruning](https://arxiv.org/pdf/1810.05270)
- jcyan 2020 AAAI [Pruning from Scratch](https://arxiv.org/pdf/1909.12579)

NAS
---
- jcyan 2019 ICLR [ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware](https://arxiv.org/pdf/1812.00332)
- jcyan 2020 ICML [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/pdf/1802.03268)
- jcyan 2020 ICLR [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://arxiv.org/pdf/1908.09791)
- jcyan 2020 CVPR [MnasNet: Platform-Aware Neural Architecture Search for Mobile](https://arxiv.org/pdf/1807.11626)
- jcyan 2020 CVPR [Designing Network Design Spaces](https://arxiv.org/pdf/2003.13678)
- jcyan 2020 CVPR [APQ: Joint Search for Network Architecture, Pruning and Quantization Policy](https://arxiv.org/pdf/2006.08509)
- jcyan 2021 ICLR [UMEC: Unified Model and Embedding Compression for Efficient Recommendation Systems](https://openreview.net/pdf?id=BM---bH_RSh)

Survey
---
- [Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey](https://ieeexplore.ieee.org/document/9043731)
